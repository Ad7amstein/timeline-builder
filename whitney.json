{
	"title": {
		"media": {
			"url": "https://b3381490.smushcdn.com/3381490/wp-content/uploads/2023/01/DALL%C2%B7E-2024-01-07-08.01.49-An-eye-catching-and-informative-lead-image-for-a-blog-about-artificial-intelligence-for-beginners.-The-image-should-visually-represent-the-concept-of-.png?lossy=2&strip=1&webp=1",
			"credit": "<a href='https://b3381490.smushcdn.com/3381490/wp-content/uploads/2023/01/DALL%C2%B7E-2024-01-07-08.01.49-An-eye-catching-and-informative-lead-image-for-a-blog-about-artificial-intelligence-for-beginners.-The-image-should-visually-represent-the-concept-of-.png?lossy=2&strip=1&webp=1'>willbhurd</a>"
		},
		"text": {
			"headline": "Notable discoveries in ai<br/> 1763 - 2017",
			"text": "<p>The history of artificial intelligence (AI) as a field is intertwined with the history of machine learning, as the algorithms and computational advances that underpin ML fed into the development of AI.</p>"
		}
	},
	"events": [
		{
			"media": {
				"url": "https://commons.wikimedia.org/wiki/File:Thomas_Bayes.gif#/media/File:Thomas_Bayes.gif",
				"caption": "<b>Thomas Bayes<b>, Only known portrait that is possibly of Bayes from a 1936 book, but it is doubtful whether the portrait is actually of him.",
				"credit": "By Unknown author - <a rel='nofollow' class='external autonumber' href='http://www.bioquest.org/products/auth_images/422_bayes.gif'>[2]</a><a rel='nofollow' class='external autonumber' href='http://www.york.ac.uk/depts/maths/histstat/images/bayes.gif'>[3]</a>, Public Domain, <a href='https://commons.wikimedia.org/w/index.php?curid=14532025'>Link</a>"
			},
			"start_date": {
				"year": "1763"
			},
			"end_date": {
				"year": "1812"
			},
			"text": {
				"headline": "Bayes' theorem",
				"text": "<p>and its predecessors. This theorem and its applications underlie inference, describing the probability of an event occurring based on prior knowledge.</p>"
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/X33-ellips-1.svg/1280px-X33-ellips-1.svg.png",
				"caption": "Conic fitting a set of points using least-squares approximation.",
				"credit": "By <a href='//commons.wikimedia.org/w/index.php?title=User:Svjo&amp;action=edit&amp;redlink=1' class='new' title='User:Svjo (page does not exist)'>Svjo</a> - <span class='int-own-work' lang='en'>Own work</span>, <a href='https://creativecommons.org/licenses/by-sa/4.0' title='Creative Commons Attribution-Share Alike 4.0'>CC BY-SA 4.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=41933103'>Link</a>"
			},
			"start_date": {
				"year": "1805"
			},
			"text": {
				"headline": "Least Square Theory",
				"text": "by French mathematician Adrien-Marie Legendre. This theory, helps in data fitting."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/7/70/AAMarkov.jpg",
				"caption": "Russian mathematician Andrey Markov",
				"credit": "By Unknown author - История Академии наук СССР. — М.−Л.: Наука, 1964. — Т. 2. — С. 475., Public Domain, <a href='https://commons.wikimedia.org/w/index.php?curid=1332494'>Link</a>"
			},
			"start_date": {
				"year": "1913"
			},
			"text": {
				"headline": "Markov Chains",
				"text": "named after Russian mathematician Andrey Markov, is used to describe a sequence of possible events based on a previous state."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/3/31/Perceptron.svg",
				"caption": "The appropriate weights are applied to the inputs, and the resulting weighted sum passed to a function that produces the output o.",
				"credit": "By <a href='https://en.wikipedia.org/wiki/User:Mat_the_w' class='extiw' title='wikipedia:User:Mat the w'>Mat the w</a> at <a href='https://en.wikipedia.org/wiki/' class='extiw' title='wikipedia:'>English Wikipedia</a>, <a href='https://creativecommons.org/licenses/by-sa/3.0' title='Creative Commons Attribution-Share Alike 3.0'>CC BY-SA 3.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=23766733'>Link</a>"
			},
			"start_date": {
				"year": "1957"
			},
			"text": {
				"headline": "Perceptron",
				"text": "is a type of linear classifier invented by American psychologist Frank Rosenblatt that underlies advances in deep learning."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/e/e7/KnnClassification.svg",
				"caption": "Example of k-NN classification. The test sample (green dot) should be classified either to blue squares or to red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).",
				"credit": "By Antti Ajanki <a href='//commons.wikimedia.org/w/index.php?title=User:AnAj&amp;action=edit&amp;redlink=1' class='new' title='User:AnAj (page does not exist)'>AnAj</a> - <span class='int-own-work' lang='en'>Own work</span>, <a href='http://creativecommons.org/licenses/by-sa/3.0/' title='Creative Commons Attribution-Share Alike 3.0'>CC BY-SA 3.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=2170282'>Link</a>"
			},
			"start_date": {
				"year": "1967"
			},
			"text": {
				"headline": "Nearest Neighbor",
				"text": "Nearest Neighbor is an algorithm originally designed to map routes. In an ML context it is used to detect patterns."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png",
				"caption": "Diagram of an artificial neural network.",
				"credit": "By <a href='//commons.wikimedia.org/wiki/User:Chrislb' title='User:Chrislb'>Chrislb</a> - created by <a href='//commons.wikimedia.org/wiki/User:Chrislb' title='User:Chrislb'>Chrislb</a>, <a href='http://creativecommons.org/licenses/by-sa/3.0/' title='Creative Commons Attribution-Share Alike 3.0'>CC BY-SA 3.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=224555'>Link</a>"
			},
			"start_date": {
				"year": "1970"
			},
			"text": {
				"headline": "Backpropagation",
				"text": "In machine learning, backpropagation is a gradient estimation method commonly used for training neural networks to compute the network parameter updates."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg",
				"caption": "Compressed (left) and unfolded (right) basic recurrent neural network.",
				"credit": "By <a href='//commons.wikimedia.org/wiki/User:Ixnay' title='User:Ixnay'>fdeloche</a> - <span class='int-own-work' lang='en'>Own work</span>, <a href='https://creativecommons.org/licenses/by-sa/4.0' title='Creative Commons Attribution-Share Alike 4.0'>CC BY-SA 4.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=60109157'>Link</a>"
			},
			"start_date": {
				"year": "1982"
			},
			"text": {
				"headline": "Recurrent neural network",
				"text": "Recurrent neural networks (RNNs) are a class of artificial neural networks for sequential data processing. Unlike feedforward neural networks, which process data in a single pass, RNNs process data across multiple time steps, making them well-adapted for modelling and processing text, speech, and time series."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png",
				"caption": "Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors.",
				"credit": "By <a href='//commons.wikimedia.org/w/index.php?title=User:Larhmam&amp;action=edit&amp;redlink=1' class='new' title='User:Larhmam (page does not exist)'>Larhmam</a> - <span class='int-own-work' lang='en'>Own work</span>, <a href='https://creativecommons.org/licenses/by-sa/4.0' title='Creative Commons Attribution-Share Alike 4.0'>CC BY-SA 4.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=73710028'>Link</a>"
			},
			"start_date": {
				"year": "1995"
			},
			"text": {
				"headline": "Support Vector Machines (SVM)",
				"text": "Introduced by Vladimir Vapnik and colleagues, SVM is a supervised learning algorithm used for classification and regression tasks, particularly effective in high-dimensional spaces."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/2/26/Deep_Learning.jpg",
				"caption": "Representing images on multiple layers of abstraction in deep learning",
				"credit": "By <a href='//commons.wikimedia.org/w/index.php?title=User:Sven_Behnke&amp;action=edit&amp;redlink=1' class='new' title='User:Sven Behnke (page does not exist)'>Sven Behnke</a> - <span class='int-own-work' lang='en'>Own work</span>, <a href='https://creativecommons.org/licenses/by-sa/4.0' title='Creative Commons Attribution-Share Alike 4.0'>CC BY-SA 4.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=82466022'>Link</a>"
			},
			"start_date": {
				"year": "2006"
			},
			"text": {
				"headline": "Deep Learning",
				"text": "Geoffrey Hinton popularized deep learning by showing that deep belief networks could be efficiently trained using unsupervised learning algorithms. This development led to breakthroughs in AI."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png",
				"caption": "Typical CNN architecture",
				"credit": "By <a href='//commons.wikimedia.org/w/index.php?title=User:Aphex34&amp;action=edit&amp;redlink=1' class='new' title='User:Aphex34 (page does not exist)'>Aphex34</a> - <span class='int-own-work' lang='en'>Own work</span>, <a href='https://creativecommons.org/licenses/by-sa/4.0' title='Creative Commons Attribution-Share Alike 4.0'>CC BY-SA 4.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=45679374'>Link</a>"
			},
			"start_date": {
				"year": "2012"
			},
			"text": {
				"headline": "Convolutional Neural Networks (CNN)",
				"text": "Though first introduced by Yann LeCun in the 1990s, CNNs gained widespread attention after AlexNet (Krizhevsky, Sutskever, Hinton) won the ImageNet challenge, sparking the deep learning revolution in computer vision."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/8/83/Generative_adversarial_network.svg",
				"caption": "An illustration of how a GAN works",
				"credit": "By Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. - <a rel='nofollow' class='external free' href='https://github.com/d2l-ai/d2l-en'>https://github.com/d2l-ai/d2l-en</a>, <a href='https://creativecommons.org/licenses/by-sa/4.0' title='Creative Commons Attribution-Share Alike 4.0'>CC BY-SA 4.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=152265649'>Link</a>"
			},
			"start_date": {
				"year": "2014"
			},
			"text": {
				"headline": "Generative Adversarial Networks (GANs)",
				"text": "Introduced by Ian Goodfellow and colleagues, GANs are a class of neural networks that learn to generate new data by pitting two networks, a generator and a discriminator, against each other."
			}
		},
		{
			"media": {
				"url": "https://upload.wikimedia.org/wikipedia/commons/3/34/Transformer%2C_full_architecture.png",
				"caption": "A standard Transformer architecture, showing on the left an encoder, and on the right a decoder. Note: it uses the pre-LN convention, which is different from the post-LN convention used in the original 2017 Transformer.",
				"credit": "By dvgodoy - <a rel='nofollow' class='external free' href='https://github.com/dvgodoy/dl-visuals/?tab=readme-ov-file'>https://github.com/dvgodoy/dl-visuals/?tab=readme-ov-file</a>, <a href='https://creativecommons.org/licenses/by/4.0' title='Creative Commons Attribution 4.0'>CC BY 4.0</a>, <a href='https://commons.wikimedia.org/w/index.php?curid=151216016'>Link</a>"
			},
			"start_date": {
				"year": "2017"
			},
			"text": {
				"headline": "Transformers",
				"text": "Introduced by Vaswani et al., the transformer model revolutionized natural language processing by using attention mechanisms to process data in parallel, leading to breakthroughs in models like BERT and GPT."
			}
		}
	]
}
